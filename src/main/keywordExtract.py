# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HVnBwf51yuWXWbWCe0qEAF73wXwsX8lL
"""

# !pip install sentence_transformers

import numpy as np
import itertools

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# doc = """
#          Supervised learning is the machine learning task of 
#          learning a function that maps an input to an output based 
#          on example input-output pairs.[1] It infers a function 
#          from labeled training data consisting of a set of 
#          training examples.[2] In supervised learning, each 
#          example is a pair consisting of an input object 
#          (typically a vector) and a desired output value (also 
#          called the supervisory signal). A supervised learning 
#          algorithm analyzes the training data and produces an 
#          inferred function, which can be used for mapping new 
#          examples. An optimal scenario will allow for the algorithm 
#          to correctly determine the class labels for unseen 
#          instances. This requires the learning algorithm to  
#          generalize from the training data to unseen situations 
#          in a 'reasonable' way (see inductive bias).
#       """
doc = """
23-1í•™ë…„ë„ 10ì£¼ì°¨ ê¸°ë„ìœ„í¬ #1
ì•ˆë…•í•˜ì„¸ìš” í•œë™ ê¸°ë„ë„¤íŠ¸ì›Œí¬, Handongâ€™s HOPE ì…ë‹ˆë‹¤. ë§¤ì£¼ ë‹¤ì–‘í•œ í•œë™ ê³µë™ì²´ì˜ ê¸°ë„ì œëª©ì„ ëª¨ì•„ ê¸°ë„í•˜ê³  í•¨ê»˜ ë‚˜ëˆ„ëŠ” Nì£¼ì°¨ ê¸°ë„ìœ„í¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ë„ì œëª©ì„ ê³µìœ í•˜ê³  í•¨ê»˜ ê¸°ë„í•¨ìœ¼ë¡œ í•œë™ì´ ê¸°ë„ë¡œ ì—°í•©ë˜ê¸¸ ì†Œë§í•©ë‹ˆë‹¤!

10ì£¼ì°¨ì—ëŠ” ì „ì „ ì„ì›ë‹¨, í•™ìƒì„ êµìœ„ì›íšŒ, í•œë™ê³µë¶€ë°©ì˜ ê¸°ë„ì œëª©ì„ ê°€ì§€ê³  í•¨ê»˜ ê¸°ë„í•´ì£¼ì„¸ìš”!

ê·¸ë¦¬ê³  ì—¬ëŸ¬ë¶„ì´ ì†í•˜ì‹  ê³µë™ì²´ì˜ ê¸°ë„ì œëª©ë„ ë‚˜ëˆ ì£¼ì„¸ìš”! (ë™ìƒˆ, ë°©ìˆœì´ë°©ëŒì´, í•™íšŒ, íŒ€, ë©ì‹¤, ë™ì•„ë¦¬ ë“±ë“±)
í•œë™ì€ ê¸°ë„í•  ë•Œ ì†Œë§ì´ ìˆìŠµë‹ˆë‹¤ğŸ¥°

**ì—¬ëŸ¬ë¶„ì´ ì†í•œ ê³µë™ì²´ì˜ ê¸°ë„ì œëª©ì„ Dm/ êµ¬ê¸€í¼ì„ í†µí•´ ë‚˜ëˆ„ì–´ì£¼ì„¸ìš”**

ì¸ìŠ¤íƒ€ê·¸ë¨: @handongs_hope
êµ¬ê¸€í¼ : [https://forms.gle/JTc9qyH8XzwmyU7z9](https://forms.gle/JTc9qyH8XzwmyU7z9)

ë¬¸ì˜ : ë¦¬ë” ê¹€ìŠ¹í˜„ 010-4649-6691"""

# 3ê°œì˜ ë‹¨ì–´ ë¬¶ìŒì¸ ë‹¨ì–´êµ¬ ì¶”ì¶œ
n_gram_range = (1, 1)
stop_words = "english"

count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])
candidates = count.get_feature_names_out()

print('trigram ê°œìˆ˜ :',len(candidates))
print('trigram ë‹¤ì„¯ê°œë§Œ ì¶œë ¥ :',candidates[:5])

model = SentenceTransformer('distilbert-base-nli-mean-tokens')
doc_embedding = model.encode([doc])
candidate_embeddings = model.encode(candidates)

top_n = 5
distances = cosine_similarity(doc_embedding, candidate_embeddings)
keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]
print(keywords)

def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):
    # ë¬¸ì„œì™€ ê° í‚¤ì›Œë“œë“¤ ê°„ì˜ ìœ ì‚¬ë„
    distances = cosine_similarity(doc_embedding, candidate_embeddings)

    # ê° í‚¤ì›Œë“œë“¤ ê°„ì˜ ìœ ì‚¬ë„
    distances_candidates = cosine_similarity(candidate_embeddings, 
                                            candidate_embeddings)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì— ê¸°ë°˜í•˜ì—¬ í‚¤ì›Œë“œë“¤ ì¤‘ ìƒìœ„ top_nê°œì˜ ë‹¨ì–´ë¥¼ pick.
    words_idx = list(distances.argsort()[0][-nr_candidates:])
    words_vals = [candidates[index] for index in words_idx]
    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]

    # ê° í‚¤ì›Œë“œë“¤ ì¤‘ì—ì„œ ê°€ì¥ ëœ ìœ ì‚¬í•œ í‚¤ì›Œë“œë“¤ê°„ì˜ ì¡°í•©ì„ ê³„ì‚°
    min_sim = np.inf
    candidate = None
    for combination in itertools.combinations(range(len(words_idx)), top_n):
        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])
        if sim < min_sim:
            candidate = combination
            min_sim = sim

    return [words_vals[idx] for idx in candidate]

max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)

max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20)

def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):

    # ë¬¸ì„œì™€ ê° í‚¤ì›Œë“œë“¤ ê°„ì˜ ìœ ì‚¬ë„ê°€ ì í˜€ìˆëŠ” ë¦¬ìŠ¤íŠ¸
    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)

    # ê° í‚¤ì›Œë“œë“¤ ê°„ì˜ ìœ ì‚¬ë„
    word_similarity = cosine_similarity(candidate_embeddings)

    # ë¬¸ì„œì™€ ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ í‚¤ì›Œë“œì˜ ì¸ë±ìŠ¤ë¥¼ ì¶”ì¶œ.
    # ë§Œì•½, 2ë²ˆ ë¬¸ì„œê°€ ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì•˜ë‹¤ë©´
    # keywords_idx = [2]
    keywords_idx = [np.argmax(word_doc_similarity)]

    # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ í‚¤ì›Œë“œì˜ ì¸ë±ìŠ¤ë¥¼ ì œì™¸í•œ ë¬¸ì„œì˜ ì¸ë±ìŠ¤ë“¤
    # ë§Œì•½, 2ë²ˆ ë¬¸ì„œê°€ ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì•˜ë‹¤ë©´
    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... ì¤‘ëµ ...]
    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]

    # ìµœê³ ì˜ í‚¤ì›Œë“œëŠ” ì´ë¯¸ ì¶”ì¶œí–ˆìœ¼ë¯€ë¡œ top_n-1ë²ˆë§Œí¼ ì•„ë˜ë¥¼ ë°˜ë³µ.
    # ex) top_n = 5ë¼ë©´, ì•„ë˜ì˜ loopëŠ” 4ë²ˆ ë°˜ë³µë¨.
    for _ in range(top_n - 1):
        candidate_similarities = word_doc_similarity[candidates_idx, :]
        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)

        # MMRì„ ê³„ì‚°
        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)
        mmr_idx = candidates_idx[np.argmax(mmr)]

        # keywords & candidatesë¥¼ ì—…ë°ì´íŠ¸
        keywords_idx.append(mmr_idx)
        candidates_idx.remove(mmr_idx)

    return [words[idx] for idx in keywords_idx]

mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)

mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)