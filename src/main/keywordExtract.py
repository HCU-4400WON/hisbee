# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HVnBwf51yuWXWbWCe0qEAF73wXwsX8lL
"""

# !pip install sentence_transformers

import numpy as np
import itertools

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# doc = """
#          Supervised learning is the machine learning task of 
#          learning a function that maps an input to an output based 
#          on example input-output pairs.[1] It infers a function 
#          from labeled training data consisting of a set of 
#          training examples.[2] In supervised learning, each 
#          example is a pair consisting of an input object 
#          (typically a vector) and a desired output value (also 
#          called the supervisory signal). A supervised learning 
#          algorithm analyzes the training data and produces an 
#          inferred function, which can be used for mapping new 
#          examples. An optimal scenario will allow for the algorithm 
#          to correctly determine the class labels for unseen 
#          instances. This requires the learning algorithm to  
#          generalize from the training data to unseen situations 
#          in a 'reasonable' way (see inductive bias).
#       """
doc = """
23-1학년도 10주차 기도위크 #1
안녕하세요 한동 기도네트워크, Handong’s HOPE 입니다. 매주 다양한 한동 공동체의 기도제목을 모아 기도하고 함께 나누는 N주차 기도위크가 시작되었습니다. 기도제목을 공유하고 함께 기도함으로 한동이 기도로 연합되길 소망합니다!

10주차에는 전전 임원단, 학생선교위원회, 한동공부방의 기도제목을 가지고 함께 기도해주세요!

그리고 여러분이 속하신 공동체의 기도제목도 나눠주세요! (동새, 방순이방돌이, 학회, 팀, 랩실, 동아리 등등)
한동은 기도할 때 소망이 있습니다🥰

**여러분이 속한 공동체의 기도제목을 Dm/ 구글폼을 통해 나누어주세요**

인스타그램: @handongs_hope
구글폼 : [https://forms.gle/JTc9qyH8XzwmyU7z9](https://forms.gle/JTc9qyH8XzwmyU7z9)

문의 : 리더 김승현 010-4649-6691"""

# 3개의 단어 묶음인 단어구 추출
n_gram_range = (1, 1)
stop_words = "english"

count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])
candidates = count.get_feature_names_out()

print('trigram 개수 :',len(candidates))
print('trigram 다섯개만 출력 :',candidates[:5])

model = SentenceTransformer('distilbert-base-nli-mean-tokens')
doc_embedding = model.encode([doc])
candidate_embeddings = model.encode(candidates)

top_n = 5
distances = cosine_similarity(doc_embedding, candidate_embeddings)
keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]
print(keywords)

def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):
    # 문서와 각 키워드들 간의 유사도
    distances = cosine_similarity(doc_embedding, candidate_embeddings)

    # 각 키워드들 간의 유사도
    distances_candidates = cosine_similarity(candidate_embeddings, 
                                            candidate_embeddings)

    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.
    words_idx = list(distances.argsort()[0][-nr_candidates:])
    words_vals = [candidates[index] for index in words_idx]
    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]

    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산
    min_sim = np.inf
    candidate = None
    for combination in itertools.combinations(range(len(words_idx)), top_n):
        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])
        if sim < min_sim:
            candidate = combination
            min_sim = sim

    return [words_vals[idx] for idx in candidate]

max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)

max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20)

def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):

    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트
    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)

    # 각 키워드들 간의 유사도
    word_similarity = cosine_similarity(candidate_embeddings)

    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.
    # 만약, 2번 문서가 가장 유사도가 높았다면
    # keywords_idx = [2]
    keywords_idx = [np.argmax(word_doc_similarity)]

    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들
    # 만약, 2번 문서가 가장 유사도가 높았다면
    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]
    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]

    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.
    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.
    for _ in range(top_n - 1):
        candidate_similarities = word_doc_similarity[candidates_idx, :]
        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)

        # MMR을 계산
        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)
        mmr_idx = candidates_idx[np.argmax(mmr)]

        # keywords & candidates를 업데이트
        keywords_idx.append(mmr_idx)
        candidates_idx.remove(mmr_idx)

    return [words[idx] for idx in keywords_idx]

mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)

mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)